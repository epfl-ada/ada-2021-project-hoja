{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pandas as pd\n",
    "from qwikidata.entity import WikidataItem\n",
    "from qwikidata.json_dump import WikidataJsonDump\n",
    "from qwikidata.utils import dump_entities_to_json\n",
    "import pywikibot\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"country_url_end.txt\", \"r\")\n",
    "url_end_dic = dict()\n",
    "for index, line in enumerate(file):\n",
    "    url_end_dic[line[:-1].split('\\t')[1][1:]] = line[:-1].split('\\t')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country_website(url, url_end_dic) -> str:  \n",
    "    \"\"\"This function finds the country in which the company of the url is based,\n",
    "    e.g. for www.guardian.co.uk it will return Great-Brittain\n",
    "    input:\n",
    "        url: str, url of which the country needs to be found\n",
    "        url_end_dic: dict, dictionary of countries for which url was already found\n",
    "    output:\n",
    "        country: str, found country\"\"\"\n",
    "    \n",
    "    country =  None\n",
    "\n",
    "    url_ending = url.split('.')[-1]\n",
    "    if url_ending in url_end_dic:\n",
    "        country = url_end_dic[url_ending]\n",
    "\n",
    "    else:\n",
    "        q_website = get_identifier(url)\n",
    "        if q_website:\n",
    "            country = get_country_from_website(q_website)\n",
    "\n",
    "        if q_website is None or country is None:\n",
    "            url_try_list = url.split('.')\n",
    "            url_try = max(url_try_list, key=len)\n",
    "            q_website = get_identifier(url_try)\n",
    "            if q_website:\n",
    "                country = get_country_from_website(q_website)\n",
    "            \n",
    "    return country\n",
    "\n",
    "def get_country_from_identifier(q_website):\n",
    "    site = pywikibot.Site(\"wikidata\", \"wikidata\")\n",
    "    repo = site.data_repository()\n",
    "    item = pywikibot.ItemPage(repo, q_website)\n",
    "    \n",
    "    if not item.isRedirectPage():\n",
    "        item_dict = item.get()\n",
    "        if \"P17\" in item_dict[\"claims\"]:\n",
    "            clm_list = item_dict[\"claims\"][\"P17\"]\n",
    "            for clm in clm_list:\n",
    "                clm_trgt = clm.getTarget()   \n",
    "\n",
    "                return clm_trgt.text[\"labels\"][\"en\"]\n",
    "\n",
    "def get_identifier(item) -> str:\n",
    "    \"\"\"This function finds the wikidata identifier for a given string input (item)\n",
    "    input:\n",
    "        -item: str, item of which you want the wikidata identfier\n",
    "    output:\n",
    "        : str, wikidata identfier\"\"\"\n",
    "    \n",
    "    params = dict (\n",
    "            action='wbsearchentities',\n",
    "            format='json',\n",
    "            language='en',\n",
    "            uselang='en',\n",
    "            type='item',\n",
    "            search=item\n",
    "            )\n",
    "\n",
    "    response = requests.get('https://www.wikidata.org/w/api.php?', params).json()\n",
    "    if response.get('search'):\n",
    "        return response.get('search')[0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country_speaker(q_person) ->list:\n",
    "    \"\"\"This function finds the nationality of a person based on \n",
    "    a wikidata identifier (Q)\n",
    "    input:\n",
    "        -q_person: str, wikidata identifier of which you want to find the country\n",
    "    output:\n",
    "        : str, country \"\"\"\n",
    "    site = pywikibot.Site(\"wikidata\", \"wikidata\")\n",
    "    repo = site.data_repository()\n",
    "    item = pywikibot.ItemPage(repo, q_person)\n",
    "    item_dict = item.get()\n",
    "    nationality = []\n",
    "    \n",
    "    if \"27\" in item_dict[\"claims\"]:\n",
    "        clm_list = item_dict[\"claims\"][\"P27\"]\n",
    "        for clm in clm_list:\n",
    "            clm_trgt = clm.getTarget() \n",
    "            nationality.append(clm_trgt.text[\"labels\"][\"en\"])\n",
    "        \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_country_to_speaker(speaker_id, speaker_country_lib):\n",
    "    if speaker_id in speaker_country_lib:\n",
    "        speaker_country = speaker_country_lib[speaker_id]\n",
    "    else:\n",
    "        speaker_country = get_country_speaker(speaker_id)\n",
    "        speaker_country_lib[speaker_id] = speaker_country #assign found country for future use\n",
    "    return speaker_country, speaker_country_lib\n",
    "        \n",
    "    \n",
    "def assign_country_to_url(url, url_country_lib):\n",
    "    if url in url_country_lib:\n",
    "        url_country = url_country_lib[url]\n",
    "    else:\n",
    "        url_country = get_country_website(url)\n",
    "        url_country_lib[speaker_id] = url_country #assign found country for future use\n",
    "    return url_country, url_country_lib\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features_to_line(line, speaker_country_lib, url_country_lib):\n",
    "    urls = line[\"urls\"]\n",
    "    country_url = list()\n",
    "    for url in urls:\n",
    "        country_url,\n",
    "        url_country_lib = country_url.append(assign_country_to_url(url, url_country_lib))\n",
    "        \n",
    "    speaker_ids = line[\"qids\"]\n",
    "    country_speaker = list()\n",
    "    for speaker_id in speaker_ids:\n",
    "        country_speaker,\n",
    "        speaker_country_lib = country_speaker.append(assign_country_to_speaker(speaker_id, speaker_country_lib))\n",
    "    \n",
    "    line[\"url_countries\"] = country_url\n",
    "    line[\"speaker_countries\"] = country_speaker\n",
    "    \n",
    "    return line, speaker_country_lib, url_country_lib\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features_to_keyword_file(df, speaker_country_lib, url_country_lib):\n",
    "    for i in range(len(df)):\n",
    "        df.loc[i], speaker_country_lib, url_country_lib = add_features_to_line(df.loc[i],\n",
    "                                                                               speaker_country_lib, url_country_lib)\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_json('C:/Users/Alberto/nutrients.json', lines=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
